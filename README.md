# Data Engineer Journey ğŸš€

Welcome to my data engineering learning journey! This repo captures my daily learning, practice, and exploration as I build my skills.

## Folder Structure

```data-engineer-journey/\
â”œâ”€â”€ day_1_pandas.py                    # Day 1 - Basic Pandas operations\
â”œâ”€â”€ day_2_etl/                         # Day 2 - ETL practice & profiling\
â”‚   â”œâ”€â”€ day_2_etl_profile_clean.py     # Cleaned profiling code\
â”‚   â””â”€â”€ imdb_1000_profile_report.html  # Pandas profiling report\
â”œâ”€â”€ README.md                          # You're here!\
```
---

## Day 1 ğŸ¼ â€” Pandas Kickoff

Started the journey by diving into **Pandas** â€” creating Series, DataFrames, doing basic indexing, filtering, grouping, and exploring real datasets.

> Baby steps with big tools â€” slow and steady wins the ETL race. ğŸ¢

---

## Day 2 ğŸ” â€” ETL & Debug Drama

We (me and ChatGPT) spent way too long wondering why files werenâ€™t showing up â€” only to realize the **Explorer sidebar section for folders was hidden under the RStudio tab** ğŸ¤¯. Dragged it up, and voilÃ  â€” all files were right there.

Also: struggled a bit with installing `pandas-profiling`, handling missing `pip`, and fixing terminal paths â€” **but we figured it out!**

> Lesson learned: **Always check your UI before blaming your code.** ğŸ˜„

---

## Day 3 ğŸ“… â€” CSV to SQLite ETL

- ğŸ—‚ï¸ Created a `books.csv` file with sample data
- ğŸ“¥ Extracted it using `pandas.read_csv`
- ğŸ§¹ Transformed the column names for SQL-friendliness
- ğŸ§± Loaded it into a local `books.db` using `sqlite3`
- ğŸ” Queried the DB using Python (and Copilot!)

This was my first full ETL cycle â€” and I now understand how raw data turns into something queryable ğŸ’ª

---

## Goals ğŸ¯

- ğŸ”„ Master real-world ETL pipelines using Python and Pandas  
- ğŸ—ƒï¸ Load and manage data using relational databases like SQLite and PostgreSQL  
- ğŸ“Š Explore and profile datasets with tools like `pandas-profiling` and `ydata-profiling`  
- ğŸ§  Strengthen SQL skills (DDL, DML, Joins, Aggregations, Subqueries, and Window functions)  
- ğŸ§° Build and automate workflows using tools like Airflow  
- ğŸ›¢ï¸ Understand data warehousing concepts and work with cloud platforms  
- ğŸ§ª Test, debug, and optimize pipelines like a real data engineer  
- ğŸ“ Document everything along the way to build a public portfolio
- ğŸ’¼ Become confident and job-ready for a backend or data engineering role
  
---

Stay tuned for more learning breakthroughs and fun debug moments!

